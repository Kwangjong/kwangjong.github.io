---
layout: post
title: "Data Science Review: Intro to ML III"
tags: blog, datascience, machinelearning
date: 2022-06-16 12:30 +0900
---

## Bagging and Boosting
In previous postings, we used the random forest method for prediction, which make prediction by averaging serveral different trees. We call this type of method that combines different machine learning models as an **ensemble method** or **ensemble learning**.

There are two major ways to combine different machine learning models in **ensemble learning**: **bagging** and **boosting**.

![bagging and boosting](https://miro.medium.com/max/700/1*zTgGBTQIMlASWm5QuS2UpA.jpeg)

**Bagging** trains its component models separately and produces prediction by voting or averaging predictions made by each component model. Random forest is one example of a bagging algorithm.

**Boosting**, on the other hand, is a method that chains component models so that each model focuses on the errors made by the previous model improving the overall performance. Gradient Boosting is one example of boosting algorithm.

## Gradient Boosting
**Gradient boosting** is a method that goes through cycles to iteratively add models to an ensemble.
As the name suggests, gradient boosting minimizes the loss function of the model by adding a new model with its parameter calculated using gradient descent. This will be repeated until loss function close to zero or specific number of model is created.

## Implementation using XGBoost
XGBoost (eXtreme Gradient Boosting) is a powerful library with an implementation of gradient boost with serveral additional features for its performance and speed.

* `n_estimators`: number of models to add to the ensemble. Typically ranges from 100-1000.

* `early_stopping_rounds`: number of rounds of deteriorization of validation score before stopping the model. The model will stop before reaching the 'n_estimators' if the model stop improving. 5 is a resonable choice.

* `learning_rate`: rate at which the predictions from each model affecting the final prediction. This allows `n_estimators` to be set with larger number without overfitting. In general, small `learning_rate` and large `n_estimators` yields accurate model, but it will take longer to train.

* `n_jobs`: number of threads. This usually equals to number of the core the machine running this calculation has. It is Useful for larger dataset. 

```python
my_model = XGBRegressor(n_estimators=1000, learning_rate=0.05, n_jobs=4)
my_model.fit(X_train, y_train, 
             early_stopping_rounds=5, 
             eval_set=[(X_valid, y_valid)], 
             verbose=False)
```


## Links
* [Ensemble Learning: Bagging & Boosting On Towards Data Science](https://towardsdatascience.com/ensemble-learning-bagging-boosting-3098079e5422)
* [Intermediate ML Course on Kaggle](https://www.kaggle.com/learn/intermediate-machine-learning)
